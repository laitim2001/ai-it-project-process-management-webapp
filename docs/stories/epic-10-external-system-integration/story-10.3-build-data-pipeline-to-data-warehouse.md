# Story 10.3: 建立數據導出至數據倉儲的管道

## User Story

**身為** 數據分析團隊,
**我想要** 有一個穩定的數據管道 (Data Pipeline)，將平台中的核心業務數據定期同步到公司的中央數據倉儲,
**以便** 我們能進行更複雜的、跨系統的商業智慧 (BI) 分析。

## 背景說明
本平台雖然提供了基礎的報告功能，但其真正的數據價值在於與其他業務系統（如銷售、客戶關係管理）的數據結合進行綜合分析。此功能旨在將本平台從一個獨立的應用，轉變為企業數據生態系統中的一個高價值數據源。透過建立一個標準化的數據導出管道，我們使數據分析師能夠利用專業的 BI 工具，從專案數據中發掘更深層次的洞察，例如「哪個區域的專案 ROI 最高？」或「IT 支出與銷售額的關聯性是什麼？」。

## 技術規格 (初步建議)
*   **後端:**
    *   需要開發一個 `DataWarehouseExportService`。
    *   需要一個定期的背景任務（e.g., a cron job running nightly）來觸發導出。
    *   導出邏輯應為「變更數據捕獲」(Change Data Capture, CDC)。即只導出從上次導出以來發生變更（新增或修改）的數據，而不是每次都全量導出。
    *   數據應被轉換為一種通用的、非正規化的格式（如 JSON Lines 或 Parquet），並上傳到一個中繼的儲存位置（如 AWS S3, Azure Blob Storage）。
    *   數據倉儲團隊可以從該中繼儲存位置，將數據載入到他們的系統中（如 Snowflake, BigQuery）。
*   **資料庫:** 核心業務表（`projects`, `budget_pools`, `expenses` 等）需要有 `created_at` 和 `updated_at` 時間戳欄位，以支持 CDC。

## 驗收標準
1.  數據導出任務應能每天成功運行一次。
2.  每次運行後，包含當天所有變更數據的檔案應出現在指定的中繼儲存位置。
3.  導出的數據格式應與預先定義的 schema 一致。
4.  如果某一天沒有任何數據變更，任務應能正常結束，不產生空文件。
5.  導出過程不應對主應用程式的正常線上服務性能產生顯著影響。

## 技術債務考量
*   **暫不包含：** 即時數據流 (Real-time data streaming)。MVP 階段採用每日批次導出的方式。
*   **未來考慮：** 整合如 Kafka 或 AWS Kinesis 等流處理平台，實現數據的近即時同步。
